---
title: Week7
date: 2024-10-26
updated: 2024-10-27
categories: life
tags:
  - 周记
top: 1
---

## Research
首先是和师先生好好聊了聊以后的研究方向，师先生的建议是做有“品味”的科研工作而不是继续这样感觉读几篇论文找找缺陷做点增量工作，比方说什么NIPS文章补补缺陷去发AAAI(说实话我刚听到时候觉得好像听起来还行？毕竟现在手上没有一篇发出来的paper)。持续学习、大模型的领域适配以及对于ood样本置信度等等问题、深度学习与符号系统的结合以及trustworthy AI，一个个听起来都让人心驰神往。然后我终于按捺不住问了一嘴：“那读博做GNN怎么样，但是感觉这两年GNN有点寄”。

“GNN确实这两年寄了”

说实话这是意料之中的答案，三大会上越来越多的文章是GNN的可解释性或者for science或者安全，GNN本身的的能力似乎瓶颈就在这里，短期内似乎看不到突破的希望。或许读博的时候还是换个方向可能好一点？至少目前而言我不觉得我能够真正取设计更好的graph machine learning model。

然后把前面WWW的课题又重新复盘了一下，老实说比我想想中还要多很多问题，尤其是我之前一直有些敷衍的把我们的非侵入式后门攻击和传统的对抗攻击区别问题。我之前只感觉其实差别就是攻击目标的问题，对抗攻击主要目标是添加扰动降低模型性能，而后门攻击主要是假发后门导致错误分类。乍一听好像明显不一样，但是对抗攻击里面target attack和我们的非侵入式后门攻击的目标其实是一样的。

遂回去好好补了补关于对抗攻击的相关内容，其实我个人觉得在攻击目标的体现上非侵入式后门攻击和对抗攻击中的target attack是差不多的。但是很明显的，对抗攻击需要的是全图的输入，但是非侵入式后门攻击需要的是简单几个节点特征，甚至这几个节点可以是随机的。这就决定了二者在实际应用时候的巨大差别，比方说在金融领域异常用户检测中，相比获得完整的图信息融合添加全局扰动，我们的非侵入式后门攻击只需要改变几个用户的信息以及他们的交易信息就可以实现。

## Life
周二终于搬进了工位。把原来寝室的4k搬了过去，买了台新的2k 100hz平替，感觉差别实在有点大，但是差别好像主要是在看网页和文档上。打怪猎时候甚至感觉没什么太大区别？刚搬过去时候才发现进组一年才申请工位好像有点抽象，还有学长那一句“经典做GNN”更加加深了对GNN要寄的刻板印象。

周三早上听了丁肇中的讲座，讲座当然是极好的，不过把前面珠峰启动仪式时间再砍一半应该体验会更好。首先当然是学到了很多，但是说实话很多经验并不是那么适用于当今AI领域，当然也可能是我懂得太少又过于武断，不过在听到那句“我拿诺奖只是因为我在特定领域有较大贡献，我在其他领域可能还不如各位，所以我从不给大家建议”时确实有一种难言的敬佩，同时觉得应该强迫二极管小南梁过来听的。

下午是AMS的面试，说实话那个mit老爷爷的口音完全听不懂，而且说实话我算是比较摆的了，别人都是脱稿过去，什么都没带，我直接把稿子带过去，还很贴心的给他也带了一份(bushi)，同时还把我的论文一并彩打了一份，为什么不打南梁的呢，因为PKDD那个单列模板导致paper有20页，20r打篇不是自己写的论文还是有点抽象。面试时候印象挺深的几个点，第一个问题好像是你用markdown干什么，当时一下子有点懵，因为在技能python、pytorch、c、c++、c#、latex、markdown里面挑最后一个确实没想到，我当时一度觉得听错了，因为markdown对我来说还行确实就是个简单的文本编辑器。后来发现可能搞物理的确实不用markdown所以好奇一下。

最有意思的是我那篇论文投的时候ACM模板没改，最后一页还留着Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009，以及首页的Conference acronym ’XX, June 03–05, 2018, Woodstock, NY。当他疑惑地问道你五岁就发表了这篇文章吗时场面一度有些尴尬，还好这句话当时听懂了并且赶紧解释了这是review version以及模板的问题。

之后的问题就比较正常了，介绍一下工作的意义以及描述一下为什么要去AMS。最后留了一个一维随机游走的距离期望计算的作业。总体来说感觉面得还行吧，不过我对自己的散装口语确实有了更加清晰地认知。

周四一天基本上都用来研究那个一维随机游走的问题了。首先是查了几篇国内博客，盲目相信了根号n的答案，遂立马写代码仿真试了一下，结果就是怎么都不对，之后很长一段时间都在研究C++ SRAND和python random函数的随机实现是不是有问题，直到仲爷看了一眼说应该不是仿真的问题，理论推导结论可能是错的。然后在stack overflow和wolfram找到了结论和相应过程，CSDN、博客园和知乎害人不浅:cry:。之后就是想办法把过程看懂然后搬过去，输入一大堆贝塞尔函数那块实在没看懂，又是后悔没好好学高数和数理方程的一天。

周五开始肝数据挖掘大作业了，找了几篇论文，一半都没开源代码，剩下来有一半都只有code will be released soon。再挑挑拣拣感觉只有两个课题好像勉强可以拿来先试一试，结果就是一个代码和数据集纯诈骗，另一个7b微调模型只在他给的那个唯一样例时候表现正常，其他时候完全不讲人话，13b模型用了各种办法塞到不同gpu里面但是总是有各种奇奇怪怪的报错。

周六一天都在想办法解决那个微调大模型的问题，但是寄。

周日终于放弃了多卡推理的想法，直接一狠心用8bit量化。然后13b模型就正常跑起来了:laughing:。再不跑起来就真要调api诈骗了。

所以别总是想什么时间复杂度太高，实现太丑陋或者精确度不高的问题，能跑起来就是胜利✌！

其他就是这段时间相比赶稿的话还是闲了不少，然后就开始想各种事情，很明显地又开始焦虑了。没有paper，只有绩点但是数理稀烂，coding能力跟没有一样。

## Study
课内计划要学的，但是……

其他的话打算这学期系统学门机器学习以及把概率论重修一遍。